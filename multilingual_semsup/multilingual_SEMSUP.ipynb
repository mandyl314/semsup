{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62QM0ZIi9KIp",
        "outputId": "17ed23cb-9a7e-4e8f-c044-e83b87744976"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KLbHLQKnSVE",
        "outputId": "72f3447d-9768-4ecc-ec71-dbdf780830b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'semsup'...\n",
            "remote: Enumerating objects: 1866, done.\u001b[K\n",
            "remote: Counting objects: 100% (260/260), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 1866 (delta 235), reused 221 (delta 210), pack-reused 1606\u001b[K\n",
            "Receiving objects: 100% (1866/1866), 20.60 MiB | 24.08 MiB/s, done.\n",
            "Resolving deltas: 100% (1398/1398), done.\n",
            "/content/semsup\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m952.4/952.4 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.6.1\n",
            "    Uninstalling setuptools-67.6.1:\n",
            "      Successfully uninstalled setuptools-67.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.1 requires setuptools>65.5.1, but you have setuptools 59.5.0 which is incompatible.\n",
            "arviz 0.15.1 requires setuptools>=60.0.0, but you have setuptools 59.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-59.5.0\n",
            "Found existing installation: torchtext 0.15.1\n",
            "Uninstalling torchtext-0.15.1:\n",
            "  Successfully uninstalled torchtext-0.15.1\n",
            "Found existing installation: torchaudio 2.0.1+cu118\n",
            "Uninstalling torchaudio-2.0.1+cu118:\n",
            "  Successfully uninstalled torchaudio-2.0.1+cu118\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/semsup\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting torch==1.9.1\n",
            "  Downloading torch-1.9.1-cp39-cp39-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m831.4/831.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning==1.4.9\n",
            "  Downloading pytorch_lightning-1.4.9-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.8/925.8 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.10.1\n",
            "  Downloading torchvision-0.10.1-cp39-cp39-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.1/22.1 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers==4.11.3\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==1.13.2\n",
            "  Downloading datasets-1.13.2-py3-none-any.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.1/287.1 kB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn==1.0\n",
            "  Downloading scikit_learn-1.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim==4.1.2\n",
            "  Downloading gensim-4.1.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonargparse==4.1.0\n",
            "  Downloading jsonargparse-4.1.0-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.4/125.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb==0.12.9\n",
            "  Downloading wandb-0.12.9-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from datasets==1.13.2->semsup==0.1.1) (2023.4.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets==1.13.2->semsup==0.1.1) (23.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets==1.13.2->semsup==0.1.1) (2.27.1)\n",
            "Collecting huggingface-hub<0.1.0,>=0.0.19\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets==1.13.2->semsup==0.1.1) (1.22.4)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets==1.13.2->semsup==0.1.1) (9.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets==1.13.2->semsup==0.1.1) (4.65.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets==1.13.2->semsup==0.1.1) (1.5.3)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim==4.1.2->semsup==0.1.1) (6.3.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim==4.1.2->semsup==0.1.1) (1.10.1)\n",
            "Requirement already satisfied: PyYAML>=3.13 in /usr/local/lib/python3.9/dist-packages (from jsonargparse==4.1.0->semsup==0.1.1) (6.0)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.4.9->semsup==0.1.1) (0.18.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.4.9->semsup==0.1.1) (4.5.0)\n",
            "Collecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning==1.4.9->semsup==0.1.1) (2.12.1)\n",
            "Collecting torchmetrics>=0.4.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.0->semsup==0.1.1) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.9/dist-packages (from scikit-learn==1.0->semsup==0.1.1) (1.2.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision==0.10.1->semsup==0.1.1) (8.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3->semsup==0.1.1) (2022.10.31)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers==4.11.3->semsup==0.1.1) (3.11.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.12.9->semsup==0.1.1) (1.16.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.4/97.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.3.0-py3-none-any.whl (19 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.3.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.12.9->semsup==0.1.1) (5.9.4)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.19.1-py2.py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.12.9->semsup==0.1.1) (3.20.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.12.9->semsup==0.1.1) (8.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.9/dist-packages (from wandb==0.12.9->semsup==0.1.1) (2.8.2)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.9/dist-packages (from wandb==0.12.9->semsup==0.1.1) (2.3)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==1.13.2->semsup==0.1.1) (22.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets==1.13.2->semsup==0.1.1) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==1.13.2->semsup==0.1.1) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==1.13.2->semsup==0.1.1) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets==1.13.2->semsup==0.1.1) (1.26.15)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (59.5.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (1.4.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (2.2.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (0.40.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (2.17.2)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.9/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (1.53.0)\n",
            "Requirement already satisfied: termcolor<3.0,>=2.2 in /usr/local/lib/python3.9/dist-packages (from yaspin>=1.0.0->wandb==0.12.9->semsup==0.1.1) (2.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets==1.13.2->semsup==0.1.1) (2022.7.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (6.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=2.2.0->pytorch-lightning==1.4.9->semsup==0.1.1) (3.2.2)\n",
            "Building wheels for collected packages: subprocess32, pathtools, sacremoses\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=d93d408a397679a3752642b5655a9d11a2730a778d5c87e04008ee41fde0ce44\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/f1/ce/3658488c09ec9d6b3037da989642c87575411f113798c90e14\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8808 sha256=73a51c17b740d9cb91dcc9c9180a0e742d5016e54c7726fcf7d32f994294699e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=588ed4282bdc7eced5204a361fb152a7c1851e9c7fbaa994e78c58c1eed373c8\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/1c/3d/46cf06718d63a32ff798a89594b61e7f345ab6b36d909ce033\n",
            "Successfully built subprocess32 pathtools sacremoses\n",
            "Installing collected packages: tokenizers, pathtools, yaspin, xxhash, torch, subprocess32, smmap, shortuuid, sentry-sdk, sacremoses, pyDeprecate, multidict, jsonargparse, frozenlist, docker-pycreds, dill, configparser, async-timeout, yarl, torchvision, torchmetrics, scikit-learn, multiprocess, huggingface-hub, gitdb, gensim, aiosignal, transformers, GitPython, aiohttp, wandb, pytorch-lightning, datasets, semsup\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.15.1+cu118\n",
            "    Uninstalling torchvision-0.15.1+cu118:\n",
            "      Successfully uninstalled torchvision-0.15.1+cu118\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.1\n",
            "    Uninstalling gensim-4.3.1:\n",
            "      Successfully uninstalled gensim-4.3.1\n",
            "  Running setup.py develop for semsup\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.9.1 which is incompatible.\n",
            "imbalanced-learn 0.10.1 requires scikit-learn>=1.0.2, but you have scikit-learn 1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.31 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 configparser-5.3.0 datasets-1.13.2 dill-0.3.6 docker-pycreds-0.4.0 frozenlist-1.3.3 gensim-4.1.2 gitdb-4.0.10 huggingface-hub-0.0.19 jsonargparse-4.1.0 multidict-6.0.4 multiprocess-0.70.14 pathtools-0.1.2 pyDeprecate-0.3.1 pytorch-lightning-1.4.9 sacremoses-0.0.53 scikit-learn-1.0 semsup-0.1.1 sentry-sdk-1.19.1 shortuuid-1.0.11 smmap-5.0.0 subprocess32-3.5.4 tokenizers-0.10.3 torch-1.9.1 torchmetrics-0.11.4 torchvision-0.10.1 transformers-4.11.3 wandb-0.12.9 xxhash-3.2.0 yarl-1.8.2 yaspin-2.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics==0.6.0\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.4/329.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from torchmetrics==0.6.0) (23.0)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.9/dist-packages (from torchmetrics==0.6.0) (1.9.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.9/dist-packages (from torchmetrics==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.1->torchmetrics==0.6.0) (4.5.0)\n",
            "Installing collected packages: torchmetrics\n",
            "  Attempting uninstall: torchmetrics\n",
            "    Found existing installation: torchmetrics 0.11.4\n",
            "    Uninstalling torchmetrics-0.11.4:\n",
            "      Successfully uninstalled torchmetrics-0.11.4\n",
            "Successfully installed torchmetrics-0.6.0\n",
            "--2023-04-16 02:39:41--  https://www.dropbox.com/s/90oxndkih67ffyh/class_descrs.zip?dl=1\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/dl/90oxndkih67ffyh/class_descrs.zip [following]\n",
            "--2023-04-16 02:39:41--  https://www.dropbox.com/s/dl/90oxndkih67ffyh/class_descrs.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucb4bb9bb9e1576c154c6606557a.dl.dropboxusercontent.com/cd/0/get/B6RbVdqbB1ytQnF4r00G4FigdPqrt8srOAmUIPXPRG-1argWm6Tvu3fpmjsHfIqUT-yuoGBQ0ole7M34ZsNI-nvRy9N5tegyVtyJViMFP88Py1uQzmn1UrAGc4tSOlvqB78aOkKLRU-2v5kn23EKdyUscvByQbHettNhTLTaCYJQCYAaZEoYduVWOuHaIMeelck/file?dl=1# [following]\n",
            "--2023-04-16 02:39:41--  https://ucb4bb9bb9e1576c154c6606557a.dl.dropboxusercontent.com/cd/0/get/B6RbVdqbB1ytQnF4r00G4FigdPqrt8srOAmUIPXPRG-1argWm6Tvu3fpmjsHfIqUT-yuoGBQ0ole7M34ZsNI-nvRy9N5tegyVtyJViMFP88Py1uQzmn1UrAGc4tSOlvqB78aOkKLRU-2v5kn23EKdyUscvByQbHettNhTLTaCYJQCYAaZEoYduVWOuHaIMeelck/file?dl=1\n",
            "Resolving ucb4bb9bb9e1576c154c6606557a.dl.dropboxusercontent.com (ucb4bb9bb9e1576c154c6606557a.dl.dropboxusercontent.com)... 162.125.65.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to ucb4bb9bb9e1576c154c6606557a.dl.dropboxusercontent.com (ucb4bb9bb9e1576c154c6606557a.dl.dropboxusercontent.com)|162.125.65.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12371983 (12M) [application/binary]\n",
            "Saving to: ‘class_descrs.zip’\n",
            "\n",
            "class_descrs.zip    100%[===================>]  11.80M  23.6MB/s    in 0.5s    \n",
            "\n",
            "2023-04-16 02:39:42 (23.6 MB/s) - ‘class_descrs.zip’ saved [12371983/12371983]\n",
            "\n",
            "Archive:  class_descrs.zip\n",
            "   creating: class_descrs/\n",
            "   creating: class_descrs/large_files2/\n",
            "  inflating: class_descrs/.DS_Store  \n",
            "   creating: class_descrs/newsgroup/\n",
            "   creating: class_descrs/class_descrs_spanish/\n",
            "   creating: class_descrs/class_descrs_chinese/\n",
            "   creating: class_descrs/class_descrs_arabic/\n",
            "   creating: class_descrs/awa/\n",
            "   creating: class_descrs/newsgroups/\n",
            "   creating: class_descrs/rcv1/\n",
            "   creating: class_descrs/cifar/\n",
            "   creating: class_descrs/class_descrs_russian/\n",
            "  inflating: class_descrs/newsgroup/combined_ng_manual_sentiment_train.labels  \n",
            "   creating: class_descrs/class_descrs_spanish/awa/\n",
            "   creating: class_descrs/class_descrs_spanish/newsgroups/\n",
            "   creating: class_descrs/class_descrs_spanish/large_files/\n",
            "   creating: class_descrs/class_descrs_spanish/cifar/\n",
            "   creating: class_descrs/class_descrs_chinese/newsgroups/\n",
            "   creating: class_descrs/class_descrs_chinese/cifar/\n",
            "   creating: class_descrs/class_descrs_arabic/newsgroups/\n",
            "   creating: class_descrs/class_descrs_arabic/cifar/\n",
            "  inflating: class_descrs/awa/awa_base_deep.labels  \n",
            "  inflating: class_descrs/awa/.DS_Store  \n",
            "  inflating: class_descrs/awa/awa_clsnames.labels  \n",
            "  inflating: class_descrs/awa/awa_base.labels  \n",
            "  inflating: class_descrs/awa/awa_attrlist.labels  \n",
            "  inflating: class_descrs/awa/google_awa_manual_test.labels  \n",
            "  inflating: class_descrs/awa/google_awa_manual_val.labels  \n",
            "  inflating: class_descrs/awa/google_awa_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/awa/google_awa_manual.labels  \n",
            "   creating: class_descrs/awa/large_files/\n",
            "  inflating: class_descrs/awa/google_awa_manual_train.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_manual_sentiment_train_descr1.labels  \n",
            "  inflating: class_descrs/newsgroups/ng_classnames.labels  \n",
            "  inflating: class_descrs/newsgroups/.DS_Store  \n",
            "  inflating: class_descrs/newsgroups/ng_superclass_classnames.labels  \n",
            "  inflating: class_descrs/newsgroups/ng_sentiment_classname.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_manual_test.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_manual_train.labels  \n",
            "  inflating: class_descrs/newsgroups/ng_base.labels  \n",
            "  inflating: class_descrs/newsgroups/ng_superclass.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_superclass_manual_train.labels  \n",
            "  inflating: class_descrs/newsgroups/ng_senti.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_manual_val.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_superclass_manual_test.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_manual_sentiment_val.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_manual_sentiment_test.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_superclass.labels  \n",
            "  inflating: class_descrs/newsgroups/combined_ng_manual_sentiment_train.labels  \n",
            "  inflating: class_descrs/rcv1/rcv1_1_description.json  \n",
            "  inflating: class_descrs/rcv1/debug_rcv1_class_names.json  \n",
            "  inflating: class_descrs/rcv1/google_ddg_combined.json  \n",
            "  inflating: class_descrs/rcv1/rcv1_only_class_name_no_template_bert.json  \n",
            "   creating: class_descrs/rcv1/s1_unseen_descriptions/\n",
            "  inflating: class_descrs/rcv1/combined_descriptions.json  \n",
            "  inflating: class_descrs/rcv1/rcv1_only_class_name_no_template.json  \n",
            "  inflating: class_descrs/rcv1/rcv1_multiple_gibberish_descriptions.json  \n",
            "  inflating: class_descrs/rcv1/ddg_rcv1_autoclean.json  \n",
            "  inflating: class_descrs/rcv1/google_rcv1_more_descriptions.json  \n",
            "  inflating: class_descrs/rcv1/rcv1_class_names.json  \n",
            "  inflating: class_descrs/rcv1/rcv1_gibberish_descriptions.json  \n",
            "  inflating: class_descrs/rcv1/google_rcv1_autoclean.json  \n",
            "   creating: class_descrs/rcv1/ablation_no_of_descriptions/\n",
            "  inflating: class_descrs/cifar/cifar100_superclass_eval_labels.labels  \n",
            "  inflating: class_descrs/cifar/cifar100_classnames_template.labels  \n",
            "  inflating: class_descrs/cifar/google_cifar100_autoclean.labels  \n",
            "  inflating: class_descrs/cifar/combined_cifar100_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/cifar/.DS_Store  \n",
            "  inflating: __MACOSX/class_descrs/cifar/._.DS_Store  \n",
            "  inflating: class_descrs/cifar/cifar_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/cifar/combined_cifar100_manual_train.labels  \n",
            "  inflating: class_descrs/cifar/google_cifar_super_manual.labels  \n",
            "  inflating: class_descrs/cifar/combined_cifar100_manual_test.labels  \n",
            "  inflating: class_descrs/cifar/cifar100_superclass_clsnames.labels  \n",
            "  inflating: class_descrs/cifar/combined_cifar100_manual_val.labels  \n",
            "  inflating: class_descrs/cifar/google_cifar_super_manual_val.labels  \n",
            "  inflating: class_descrs/cifar/cifar_super_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/cifar/google_cifar_super_manual_test.labels  \n",
            "  inflating: class_descrs/cifar/google_cifar_super_manual_train.labels  \n",
            "   creating: class_descrs/class_descrs_russian/newsgroups/\n",
            "   creating: class_descrs/class_descrs_russian/cifar/\n",
            "  inflating: class_descrs/class_descrs_spanish/awa/awa_base_deep.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/awa_clsnames.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/awa_base.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/awa_attrlist.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/google_awa_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/google_awa_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/google_awa_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/google_awa_manual.labels  \n",
            "   creating: class_descrs/class_descrs_spanish/awa/large_files/\n",
            "  inflating: class_descrs/class_descrs_spanish/awa/google_awa_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_manual_sentiment_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/ng_classnames.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/ng_superclass_classnames.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/ng_sentiment_classname.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/ng_base.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/ng_superclass.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_superclass_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/ng_senti.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_superclass_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_manual_sentiment_val.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_manual_sentiment_test.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_superclass.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/newsgroups/combined_ng_manual_sentiment_train.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_train.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_train.json  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_val.json  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_val.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_test.json  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_val_bov.json  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_test_bov.json  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_train_bov.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50.json  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_val_bov.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_test_bov.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25.json  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_train_bov.json  \n",
            "  inflating: class_descrs/class_descrs_spanish/large_files/awa_deep_samp50_perm25_test.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/cifar100_superclass_eval_labels.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/cifar100_classnames_template.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/google_cifar100_autoclean.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/combined_cifar100_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/cifar_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/combined_cifar100_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/google_cifar_super_manual.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/combined_cifar100_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/cifar100_superclass_clsnames.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/combined_cifar100_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/google_cifar_super_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/cifar_super_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/google_cifar_super_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/cifar/google_cifar_super_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_manual_sentiment_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/ng_classnames.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/ng_superclass_classnames.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/ng_sentiment_classname.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/ng_base.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/ng_superclass.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_superclass_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/ng_senti.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_superclass_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_manual_sentiment_val.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_manual_sentiment_test.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_superclass.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/newsgroups/combined_ng_manual_sentiment_train.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/cifar100_superclass_eval_labels.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/cifar100_classnames_template.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/google_cifar100_autoclean.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/combined_cifar100_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/cifar_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/combined_cifar100_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/google_cifar_super_manual.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/combined_cifar100_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/cifar100_superclass_clsnames.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/combined_cifar100_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/google_cifar_super_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/cifar_super_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/google_cifar_super_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_chinese/cifar/google_cifar_super_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_manual_sentiment_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/ng_classnames.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/ng_superclass_classnames.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/ng_sentiment_classname.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/ng_base.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/ng_superclass.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_superclass_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/ng_senti.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_superclass_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_manual_sentiment_val.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_manual_sentiment_test.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_superclass.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/newsgroups/combined_ng_manual_sentiment_train.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/cifar100_superclass_eval_labels.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/cifar100_classnames_template.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/google_cifar100_autoclean.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/combined_cifar100_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/cifar_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/combined_cifar100_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/google_cifar_super_manual.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/combined_cifar100_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/cifar100_superclass_clsnames.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/combined_cifar100_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/google_cifar_super_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/cifar_super_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/google_cifar_super_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_arabic/cifar/google_cifar_super_manual_train.labels  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_train.labels  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_train.json  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_val.json  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25.labels  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_val.labels  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_test.json  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_val_bov.json  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_test_bov.json  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50.labels  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_train_bov.labels  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50.json  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_val_bov.labels  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_test_bov.labels  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25.json  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_train_bov.json  \n",
            "  inflating: class_descrs/awa/large_files/awa_deep_samp50_perm25_test.labels  \n",
            "  inflating: class_descrs/rcv1/s1_unseen_descriptions/train_descriptions_augmented.json  \n",
            "  inflating: class_descrs/rcv1/s1_unseen_descriptions/train_descriptions_n_1.json  \n",
            "  inflating: class_descrs/rcv1/s1_unseen_descriptions/val_descriptions_no_name.json  \n",
            "  inflating: class_descrs/rcv1/s1_unseen_descriptions/val_descriptions.json  \n",
            "  inflating: class_descrs/rcv1/s1_unseen_descriptions/train_descriptions.json  \n",
            "  inflating: class_descrs/rcv1/ablation_no_of_descriptions/train_descriptions_30_cleaned.json  \n",
            "  inflating: class_descrs/rcv1/ablation_no_of_descriptions/train_descriptions_n_1.json  \n",
            "  inflating: class_descrs/rcv1/ablation_no_of_descriptions/train_descriptions_5.json  \n",
            "  inflating: class_descrs/rcv1/ablation_no_of_descriptions/val_descriptions.json  \n",
            "  inflating: class_descrs/rcv1/ablation_no_of_descriptions/train_descriptions_30.json  \n",
            "  inflating: class_descrs/rcv1/ablation_no_of_descriptions/train_descriptions.json  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_manual_sentiment_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/ng_classnames.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/ng_superclass_classnames.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/ng_sentiment_classname.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/ng_base.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/ng_superclass.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_superclass_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/ng_senti.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_superclass_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_manual_sentiment_val.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_manual_sentiment_test.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_superclass.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/newsgroups/combined_ng_manual_sentiment_train.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/cifar100_superclass_eval_labels.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/cifar100_classnames_template.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/google_cifar100_autoclean.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/combined_cifar100_manual_train_descr1.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/cifar_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/combined_cifar100_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/google_cifar_super_manual.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/combined_cifar100_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/cifar100_superclass_clsnames.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/combined_cifar100_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/google_cifar_super_manual_val.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/cifar_super_classnames_notemplate.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/google_cifar_super_manual_test.labels  \n",
            "  inflating: class_descrs/class_descrs_russian/cifar/google_cifar_super_manual_train.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/large_files/awa_deep_samp50_perm25_train.labels  \n",
            "  inflating: class_descrs/class_descrs_spanish/awa/large_files/awa_deep_samp50_perm25_val.labels  \n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mandyl314/semsup\n",
        "%cd semsup/\n",
        "!pip install setuptools==59.5.0\n",
        "!pip uninstall --yes torchtext torchaudio # not compatible with our version of torch\n",
        "!pip install -e .\n",
        "!pip install torchmetrics==0.6.0\n",
        "!bash download.sh\n",
        "!mkdir data_cache\n",
        "# RESTART RUNTIME TO UPDATE NEWLY INSTALLED MODULES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrW14wKenU6K",
        "outputId": "9f52b4a5-3361-4b31-ecb6-c3ef99153a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/semsup/run_cifar\n",
            "2023-04-16 02:39:55.287291: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-16 02:39:57.189014: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Global seed set to 1\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ../data_cache/cifar-100-python.tar.gz\n",
            "169001984it [00:05, 29432332.15it/s]                   \n",
            "Extracting ../data_cache/cifar-100-python.tar.gz to ../data_cache\n",
            "Downloading: 100% 466/466 [00:00<00:00, 415kB/s]\n",
            "Downloading: 100% 517M/517M [00:07<00:00, 75.3MB/s]\n",
            "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 320MB/s]\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.14.2 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "2023-04-16 02:40:51.492882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcifar_super_rus_ar_s1\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View project at \u001b[34m\u001b[4mhttps://wandb.ai/mandyl314/semsup\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  View run at \u001b[34m\u001b[4mhttps://wandb.ai/mandyl314/semsup/runs/tvpqyv2t\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /content/semsup/run_cifar/wandb/run-20230416_024049-tvpqyv2t\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\n",
            "\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "Downloading: 100% 29.0/29.0 [00:00<00:00, 29.9kB/s]\n",
            "Downloading: 100% 972k/972k [00:00<00:00, 3.02MB/s]\n",
            "Downloading: 100% 1.87M/1.87M [00:00<00:00, 21.5MB/s]\n",
            "Using custom data configuration default-2a7ce7dab485c72e\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-2a7ce7dab485c72e/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426...\n",
            "100% 1/1 [00:00<00:00, 6533.18it/s]\n",
            "100% 1/1 [00:00<00:00, 1108.43it/s]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-2a7ce7dab485c72e/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426. Subsequent calls will reuse this data.\n",
            "Parameter 'function'=<function SemSupDataModule.prepare_label_data.<locals>.<lambda> at 0x7f87d8f82dc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
            "100% 2/2 [00:00<00:00,  9.96ba/s]\n",
            "Using custom data configuration default-a2f019068b29adae\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-a2f019068b29adae/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426...\n",
            "100% 1/1 [00:00<00:00, 6732.43it/s]\n",
            "100% 1/1 [00:00<00:00, 1356.94it/s]\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a2f019068b29adae/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 62.47ba/s]\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "100% 1165/1165 [00:00<00:00, 3467.35ex/s]\n",
            "100% 149/149 [00:00<00:00, 1387.71ex/s]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name        | Type            | Params\n",
            "------------------------------------------------\n",
            "0 | label_model | DistilBertModel | 134 M \n",
            "1 | model       | ResNet          | 11.6 M\n",
            "2 | accuracy    | Accuracy        | 0     \n",
            "------------------------------------------------\n",
            "146 M     Trainable params\n",
            "0         Non-trainable params\n",
            "146 M     Total params\n",
            "585.185   Total estimated model params size (MB)\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Global seed set to 1\n",
            "/usr/local/lib/python3.9/dist-packages/pytorch_lightning/trainer/data_loading.py:326: UserWarning: The number of training samples (625) is smaller than the logging interval Trainer(log_every_n_steps=1000). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
            "  rank_zero_warn(\n",
            "Epoch 0:   0% 0/704 [00:00<00:00, 4529.49it/s]  huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 0:  91% 640/704 [12:46<01:16,  1.20s/it, loss=2.6, v_num=yv2t, train_loss=2.710]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 0:  94% 660/704 [12:47<00:51,  1.16s/it, loss=2.6, v_num=yv2t, train_loss=2.710]\n",
            "Epoch 0:  97% 680/704 [12:49<00:27,  1.13s/it, loss=2.6, v_num=yv2t, train_loss=2.710]\n",
            "Epoch 0:  99% 700/704 [12:50<00:04,  1.10s/it, loss=2.6, v_num=yv2t, train_loss=2.710]\n",
            "Epoch 0: 100% 704/704 [12:51<00:00,  1.09s/it, loss=2.55, v_num=yv2t, train_loss=2.440, val_loss=2.010, val_acc=0.280]\n",
            "Epoch 1:   0% 0/704 [00:00<00:00, 2441.39it/s, loss=2.55, v_num=yv2t, train_loss=2.440, val_loss=2.010, val_acc=0.280] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 1:  91% 640/704 [12:52<01:17,  1.21s/it, loss=1.73, v_num=yv2t, train_loss=1.700, val_loss=2.010, val_acc=0.280]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 1:  94% 660/704 [12:54<00:51,  1.17s/it, loss=1.73, v_num=yv2t, train_loss=1.700, val_loss=2.010, val_acc=0.280]\n",
            "Epoch 1:  97% 680/704 [12:55<00:27,  1.14s/it, loss=1.73, v_num=yv2t, train_loss=1.700, val_loss=2.010, val_acc=0.280]\n",
            "Epoch 1:  99% 700/704 [12:57<00:04,  1.11s/it, loss=1.73, v_num=yv2t, train_loss=1.700, val_loss=2.010, val_acc=0.280]\n",
            "Epoch 1: 100% 704/704 [12:58<00:00,  1.10s/it, loss=1.67, v_num=yv2t, train_loss=1.730, val_loss=2.260, val_acc=0.243]\n",
            "Epoch 2:   0% 0/704 [00:00<00:00, 2928.98it/s, loss=1.67, v_num=yv2t, train_loss=1.730, val_loss=2.260, val_acc=0.243] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 2:  91% 640/704 [12:52<01:17,  1.20s/it, loss=1.44, v_num=yv2t, train_loss=1.700, val_loss=2.260, val_acc=0.243]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 2:  94% 660/704 [12:53<00:51,  1.17s/it, loss=1.44, v_num=yv2t, train_loss=1.700, val_loss=2.260, val_acc=0.243]\n",
            "Epoch 2:  97% 680/704 [12:54<00:27,  1.14s/it, loss=1.44, v_num=yv2t, train_loss=1.700, val_loss=2.260, val_acc=0.243]\n",
            "Epoch 2:  99% 700/704 [12:55<00:04,  1.11s/it, loss=1.44, v_num=yv2t, train_loss=1.700, val_loss=2.260, val_acc=0.243]\n",
            "Epoch 2: 100% 704/704 [12:57<00:00,  1.10s/it, loss=1.41, v_num=yv2t, train_loss=1.270, val_loss=2.140, val_acc=0.290]\n",
            "Epoch 3:   0% 0/704 [00:00<00:00, 2480.37it/s, loss=1.41, v_num=yv2t, train_loss=1.270, val_loss=2.140, val_acc=0.290] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 3:  91% 640/704 [12:53<01:17,  1.21s/it, loss=1.18, v_num=yv2t, train_loss=0.907, val_loss=2.140, val_acc=0.290]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 3:  94% 660/704 [12:55<00:51,  1.17s/it, loss=1.18, v_num=yv2t, train_loss=0.907, val_loss=2.140, val_acc=0.290]\n",
            "Epoch 3:  97% 680/704 [12:56<00:27,  1.14s/it, loss=1.18, v_num=yv2t, train_loss=0.907, val_loss=2.140, val_acc=0.290]\n",
            "Epoch 3:  99% 700/704 [12:57<00:04,  1.11s/it, loss=1.18, v_num=yv2t, train_loss=0.907, val_loss=2.140, val_acc=0.290]\n",
            "Epoch 3: 100% 704/704 [12:58<00:00,  1.10s/it, loss=1.19, v_num=yv2t, train_loss=1.440, val_loss=2.310, val_acc=0.266]\n",
            "Epoch 4:   0% 0/704 [00:00<00:00, 3587.94it/s, loss=1.19, v_num=yv2t, train_loss=1.440, val_loss=2.310, val_acc=0.266] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 4:  91% 640/704 [12:52<01:17,  1.21s/it, loss=0.994, v_num=yv2t, train_loss=0.920, val_loss=2.310, val_acc=0.266]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 4:  94% 660/704 [12:54<00:51,  1.17s/it, loss=0.994, v_num=yv2t, train_loss=0.920, val_loss=2.310, val_acc=0.266]\n",
            "Epoch 4:  97% 680/704 [12:55<00:27,  1.14s/it, loss=0.994, v_num=yv2t, train_loss=0.920, val_loss=2.310, val_acc=0.266]\n",
            "Epoch 4:  99% 700/704 [12:56<00:04,  1.11s/it, loss=0.994, v_num=yv2t, train_loss=0.920, val_loss=2.310, val_acc=0.266]\n",
            "Epoch 4: 100% 704/704 [12:57<00:00,  1.10s/it, loss=0.967, v_num=yv2t, train_loss=0.774, val_loss=2.180, val_acc=0.298]\n",
            "Epoch 5:   0% 0/704 [00:00<00:00, 2024.28it/s, loss=0.967, v_num=yv2t, train_loss=0.774, val_loss=2.180, val_acc=0.298] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 5:  91% 640/704 [12:54<01:17,  1.21s/it, loss=0.868, v_num=yv2t, train_loss=0.890, val_loss=2.180, val_acc=0.298]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 5:  94% 660/704 [12:55<00:51,  1.17s/it, loss=0.868, v_num=yv2t, train_loss=0.890, val_loss=2.180, val_acc=0.298]\n",
            "Epoch 5:  97% 680/704 [12:56<00:27,  1.14s/it, loss=0.868, v_num=yv2t, train_loss=0.890, val_loss=2.180, val_acc=0.298]\n",
            "Epoch 5:  99% 700/704 [12:57<00:04,  1.11s/it, loss=0.868, v_num=yv2t, train_loss=0.890, val_loss=2.180, val_acc=0.298]\n",
            "Epoch 5: 100% 704/704 [12:59<00:00,  1.11s/it, loss=0.881, v_num=yv2t, train_loss=1.090, val_loss=2.190, val_acc=0.271]\n",
            "Epoch 6:   0% 0/704 [00:00<00:00, 3498.17it/s, loss=0.881, v_num=yv2t, train_loss=1.090, val_loss=2.190, val_acc=0.271] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 6:  91% 640/704 [12:52<01:17,  1.21s/it, loss=0.807, v_num=yv2t, train_loss=1.290, val_loss=2.190, val_acc=0.271]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 6:  94% 660/704 [12:54<00:51,  1.17s/it, loss=0.807, v_num=yv2t, train_loss=1.290, val_loss=2.190, val_acc=0.271]\n",
            "Epoch 6:  97% 680/704 [12:55<00:27,  1.14s/it, loss=0.807, v_num=yv2t, train_loss=1.290, val_loss=2.190, val_acc=0.271]\n",
            "Epoch 6:  99% 700/704 [12:56<00:04,  1.11s/it, loss=0.807, v_num=yv2t, train_loss=1.290, val_loss=2.190, val_acc=0.271]\n",
            "Epoch 6: 100% 704/704 [12:57<00:00,  1.10s/it, loss=0.801, v_num=yv2t, train_loss=0.628, val_loss=2.300, val_acc=0.281]\n",
            "Epoch 7:   0% 0/704 [00:00<00:00, 3833.92it/s, loss=0.801, v_num=yv2t, train_loss=0.628, val_loss=2.300, val_acc=0.281] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 7:  91% 640/704 [12:53<01:17,  1.21s/it, loss=0.761, v_num=yv2t, train_loss=0.683, val_loss=2.300, val_acc=0.281]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 7:  94% 660/704 [12:55<00:51,  1.17s/it, loss=0.761, v_num=yv2t, train_loss=0.683, val_loss=2.300, val_acc=0.281]\n",
            "Epoch 7:  97% 680/704 [12:56<00:27,  1.14s/it, loss=0.761, v_num=yv2t, train_loss=0.683, val_loss=2.300, val_acc=0.281]\n",
            "Epoch 7:  99% 700/704 [12:57<00:04,  1.11s/it, loss=0.761, v_num=yv2t, train_loss=0.683, val_loss=2.300, val_acc=0.281]\n",
            "Epoch 7: 100% 704/704 [12:58<00:00,  1.10s/it, loss=0.728, v_num=yv2t, train_loss=0.639, val_loss=2.220, val_acc=0.291]\n",
            "Epoch 8:   0% 0/704 [00:00<00:00, 2523.65it/s, loss=0.728, v_num=yv2t, train_loss=0.639, val_loss=2.220, val_acc=0.291] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 8:  91% 640/704 [12:53<01:17,  1.21s/it, loss=0.64, v_num=yv2t, train_loss=0.534, val_loss=2.220, val_acc=0.291]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 8:  94% 660/704 [12:54<00:51,  1.17s/it, loss=0.64, v_num=yv2t, train_loss=0.534, val_loss=2.220, val_acc=0.291]\n",
            "Epoch 8:  97% 680/704 [12:56<00:27,  1.14s/it, loss=0.64, v_num=yv2t, train_loss=0.534, val_loss=2.220, val_acc=0.291]\n",
            "Epoch 8:  99% 700/704 [12:57<00:04,  1.11s/it, loss=0.64, v_num=yv2t, train_loss=0.534, val_loss=2.220, val_acc=0.291]\n",
            "Epoch 8: 100% 704/704 [12:58<00:00,  1.10s/it, loss=0.62, v_num=yv2t, train_loss=0.462, val_loss=2.290, val_acc=0.283]\n",
            "Epoch 9:   0% 0/704 [00:00<00:00, 3560.53it/s, loss=0.62, v_num=yv2t, train_loss=0.462, val_loss=2.290, val_acc=0.283] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 9:  91% 640/704 [12:53<01:17,  1.21s/it, loss=0.545, v_num=yv2t, train_loss=0.735, val_loss=2.290, val_acc=0.283]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 9:  94% 660/704 [12:54<00:51,  1.17s/it, loss=0.545, v_num=yv2t, train_loss=0.735, val_loss=2.290, val_acc=0.283]\n",
            "Epoch 9:  97% 680/704 [12:56<00:27,  1.14s/it, loss=0.545, v_num=yv2t, train_loss=0.735, val_loss=2.290, val_acc=0.283]\n",
            "Epoch 9:  99% 700/704 [12:57<00:04,  1.11s/it, loss=0.545, v_num=yv2t, train_loss=0.735, val_loss=2.290, val_acc=0.283]\n",
            "Epoch 9: 100% 704/704 [12:58<00:00,  1.10s/it, loss=0.565, v_num=yv2t, train_loss=0.593, val_loss=2.350, val_acc=0.296]\n",
            "Epoch 10:   0% 0/704 [00:00<00:00, 2966.27it/s, loss=0.565, v_num=yv2t, train_loss=0.593, val_loss=2.350, val_acc=0.296]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 10:  91% 640/704 [12:53<01:17,  1.21s/it, loss=0.482, v_num=yv2t, train_loss=0.696, val_loss=2.350, val_acc=0.296]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 10:  94% 660/704 [12:54<00:51,  1.17s/it, loss=0.482, v_num=yv2t, train_loss=0.696, val_loss=2.350, val_acc=0.296]\n",
            "Epoch 10:  97% 680/704 [12:56<00:27,  1.14s/it, loss=0.482, v_num=yv2t, train_loss=0.696, val_loss=2.350, val_acc=0.296]\n",
            "Epoch 10:  99% 700/704 [12:57<00:04,  1.11s/it, loss=0.482, v_num=yv2t, train_loss=0.696, val_loss=2.350, val_acc=0.296]\n",
            "Epoch 10: 100% 704/704 [12:58<00:00,  1.10s/it, loss=0.49, v_num=yv2t, train_loss=0.436, val_loss=2.420, val_acc=0.293] \n",
            "Epoch 11:   0% 0/704 [00:00<00:00, 4088.02it/s, loss=0.49, v_num=yv2t, train_loss=0.436, val_loss=2.420, val_acc=0.293] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 11:  91% 640/704 [12:53<01:17,  1.21s/it, loss=0.408, v_num=yv2t, train_loss=0.399, val_loss=2.420, val_acc=0.293]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 11:  94% 660/704 [12:54<00:51,  1.17s/it, loss=0.408, v_num=yv2t, train_loss=0.399, val_loss=2.420, val_acc=0.293]\n",
            "Epoch 11:  97% 680/704 [12:55<00:27,  1.14s/it, loss=0.408, v_num=yv2t, train_loss=0.399, val_loss=2.420, val_acc=0.293]\n",
            "Epoch 11:  99% 700/704 [12:56<00:04,  1.11s/it, loss=0.408, v_num=yv2t, train_loss=0.399, val_loss=2.420, val_acc=0.293]\n",
            "Epoch 11: 100% 704/704 [12:58<00:00,  1.10s/it, loss=0.401, v_num=yv2t, train_loss=0.416, val_loss=2.520, val_acc=0.289]\n",
            "Epoch 12:   0% 0/704 [00:00<00:00, 3698.68it/s, loss=0.401, v_num=yv2t, train_loss=0.416, val_loss=2.520, val_acc=0.289] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 12:  91% 640/704 [12:52<01:17,  1.21s/it, loss=0.411, v_num=yv2t, train_loss=0.311, val_loss=2.520, val_acc=0.289]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 12:  94% 660/704 [12:54<00:51,  1.17s/it, loss=0.411, v_num=yv2t, train_loss=0.311, val_loss=2.520, val_acc=0.289]\n",
            "Epoch 12:  97% 680/704 [12:55<00:27,  1.14s/it, loss=0.411, v_num=yv2t, train_loss=0.311, val_loss=2.520, val_acc=0.289]\n",
            "Epoch 12:  99% 700/704 [12:57<00:04,  1.11s/it, loss=0.411, v_num=yv2t, train_loss=0.311, val_loss=2.520, val_acc=0.289]\n",
            "Epoch 12: 100% 704/704 [12:58<00:00,  1.10s/it, loss=0.432, v_num=yv2t, train_loss=0.477, val_loss=2.390, val_acc=0.263]\n",
            "Epoch 13:   0% 0/704 [00:00<00:00, 2462.89it/s, loss=0.432, v_num=yv2t, train_loss=0.477, val_loss=2.390, val_acc=0.263] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 13:  91% 640/704 [12:53<01:17,  1.21s/it, loss=0.376, v_num=yv2t, train_loss=0.466, val_loss=2.390, val_acc=0.263]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/79 [00:00<?, ?it/s]\u001b[Ahuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "\n",
            "Epoch 13:  94% 660/704 [12:54<00:51,  1.17s/it, loss=0.376, v_num=yv2t, train_loss=0.466, val_loss=2.390, val_acc=0.263]\n",
            "Epoch 13:  97% 680/704 [12:56<00:27,  1.14s/it, loss=0.376, v_num=yv2t, train_loss=0.466, val_loss=2.390, val_acc=0.263]\n",
            "Epoch 13:  99% 700/704 [12:57<00:04,  1.11s/it, loss=0.376, v_num=yv2t, train_loss=0.466, val_loss=2.390, val_acc=0.263]\n",
            "Epoch 13: 100% 704/704 [12:58<00:00,  1.10s/it, loss=0.378, v_num=yv2t, train_loss=0.388, val_loss=2.470, val_acc=0.286]\n",
            "Epoch 14:   0% 0/704 [00:00<00:00, 3421.13it/s, loss=0.378, v_num=yv2t, train_loss=0.388, val_loss=2.470, val_acc=0.286] huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch 14:  77% 540/704 [11:07<03:22,  1.23s/it, loss=0.325, v_num=yv2t, train_loss=0.336, val_loss=2.470, val_acc=0.286]"
          ]
        }
      ],
      "source": [
        "%cd ./semsup/run_cifar\n",
        "!bash run_cifar_semsup.sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwnybDR32IoC"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/semsup/checkpoints_ng_scen3_eng2.zip /content/semsup/checkpoints/\n",
        "!cp /content/semsup/checkpoints_ng_scen3_eng2.zip /content/drive/MyDrive/thesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86-TdEU0BeEK",
        "outputId": "6561eeee-2a2e-43de-80a9-e43c54dd6196"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Mar 17 18:51:21 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    24W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgbaD_hQBvR0",
        "outputId": "9c438f80-4d71-4207-c007-76f6f3f0f32c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-d855b2a1-3036-6eac-fea7-e740e9520f8d)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pppfcK0FFGC0"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/semsup/checkpoints_ng_scen1_ru2.zip /content/semsup/checkpoints/\n",
        "!cp /content/semsup/checkpoints_ng_scen1_ru2.zip /content/drive/MyDrive/thesis"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}